# UzBerta
Pre-prepared model in Uzbek (Cyrillic and latin script) to model the masked language and predict the next sentences.  UzBERT model was pretrained on ≈2M news articles (≈3Gb).
